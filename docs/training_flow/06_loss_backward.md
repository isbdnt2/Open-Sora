# 06. 损失计算与反向传播 (Loss & Backward)

## 流程图

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                              损失计算与优化流程                                      │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│   ┌─────────────────────────────────────────────────────────────────────────────┐   │
│   │                          1. 损失计算                                         │   │
│   ├─────────────────────────────────────────────────────────────────────────────┤   │
│   │                                                                             │   │
│   │   ┌─────────────┐           ┌─────────────┐                                 │   │
│   │   │   v_pred    │           │  v_target   │                                 │   │
│   │   │ [B, N, 64]  │           │ [B, N, 64]  │                                 │   │
│   │   └──────┬──────┘           └──────┬──────┘                                 │   │
│   │          │                         │                                        │   │
│   │          │         MSE Loss        │                                        │   │
│   │          └──────────┬──────────────┘                                        │   │
│   │                     │                                                       │   │
│   │                     ▼                                                       │   │
│   │          ┌─────────────────────┐                                            │   │
│   │          │ loss = mean((v_pred │                                            │   │
│   │          │   - v_target)²)     │                                            │   │
│   │          └──────────┬──────────┘                                            │   │
│   │                     │                                                       │   │
│   │                     ▼                                                       │   │
│   │          ┌─────────────────────┐                                            │   │
│   │          │    I2V 掩码处理      │   (可选)                                   │   │
│   │          │  排除参考帧的损失    │                                            │   │
│   │          └──────────┬──────────┘                                            │   │
│   │                     │                                                       │   │
│   │                     ▼                                                       │   │
│   │          ┌─────────────────────┐                                            │   │
│   │          │   final_loss        │                                            │   │
│   │          │      scalar         │                                            │   │
│   │          └─────────────────────┘                                            │   │
│   │                                                                             │   │
│   └─────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│   ┌─────────────────────────────────────────────────────────────────────────────┐   │
│   │                          2. 反向传播                                         │   │
│   ├─────────────────────────────────────────────────────────────────────────────┤   │
│   │                                                                             │   │
│   │   ┌──────────────────────────────────────────────────────────────────────┐  │   │
│   │   │                                                                      │  │   │
│   │   │   loss.backward()                                                    │  │   │
│   │   │       │                                                              │  │   │
│   │   │       ▼                                                              │  │   │
│   │   │   ┌─────────────────────────────────────────────────────────────┐    │  │   │
│   │   │   │               Gradient Checkpointing                        │    │  │   │
│   │   │   │                                                             │    │  │   │
│   │   │   │   if grad_ckpt = True:                                      │    │  │   │
│   │   │   │       - 前向时: 只保存每个 block 的输入激活值                │    │  │   │
│   │   │   │       - 反向时: 重新计算 block 内部激活值                    │    │  │   │
│   │   │   │       - 节省约 60% 的激活内存                                │    │  │   │
│   │   │   │                                                             │    │  │   │
│   │   │   └─────────────────────────────────────────────────────────────┘    │  │   │
│   │   │       │                                                              │  │   │
│   │   │       ▼                                                              │  │   │
│   │   │   ┌─────────────────────────────────────────────────────────────┐    │  │   │
│   │   │   │               ZeRO-2 Gradient Sharding                      │    │  │   │
│   │   │   │                                                             │    │  │   │
│   │   │   │   梯度分片到各 GPU:                                         │    │  │   │
│   │   │   │   - 每个 GPU 只存储 1/world_size 的梯度                     │    │  │   │
│   │   │   │   - Reduce-Scatter 聚合梯度                                 │    │  │   │
│   │   │   │                                                             │    │  │   │
│   │   │   └─────────────────────────────────────────────────────────────┘    │  │   │
│   │   │                                                                      │  │   │
│   │   └──────────────────────────────────────────────────────────────────────┘  │   │
│   │                                                                             │   │
│   └─────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│   ┌─────────────────────────────────────────────────────────────────────────────┐   │
│   │                          3. 梯度裁剪                                         │   │
│   ├─────────────────────────────────────────────────────────────────────────────┤   │
│   │                                                                             │   │
│   │   ┌──────────────────────────────────────────────────────────────────────┐  │   │
│   │   │                                                                      │  │   │
│   │   │   gradients                                                          │  │   │
│   │   │       │                                                              │  │   │
│   │   │       ▼                                                              │  │   │
│   │   │   ┌─────────────────────────────────────────────────────────────┐    │  │   │
│   │   │   │           Gradient Clipping (max_norm=1.0)                  │    │  │   │
│   │   │   │                                                             │    │  │   │
│   │   │   │   total_norm = sqrt(sum(grad.norm()² for all params))       │    │  │   │
│   │   │   │                                                             │    │  │   │
│   │   │   │   if total_norm > max_norm:                                 │    │  │   │
│   │   │   │       clip_coef = max_norm / total_norm                     │    │  │   │
│   │   │   │       for param in model.parameters():                      │    │  │   │
│   │   │   │           param.grad *= clip_coef                           │    │  │   │
│   │   │   │                                                             │    │  │   │
│   │   │   └─────────────────────────────────────────────────────────────┘    │  │   │
│   │   │       │                                                              │  │   │
│   │   │       ▼                                                              │  │   │
│   │   │   clipped_gradients                                                  │  │   │
│   │   │                                                                      │  │   │
│   │   └──────────────────────────────────────────────────────────────────────┘  │   │
│   │                                                                             │   │
│   └─────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│   ┌─────────────────────────────────────────────────────────────────────────────┐   │
│   │                          4. 参数更新                                         │   │
│   ├─────────────────────────────────────────────────────────────────────────────┤   │
│   │                                                                             │   │
│   │   ┌──────────────────────────────────────────────────────────────────────┐  │   │
│   │   │                      AdamW Optimizer                                 │  │   │
│   │   │                                                                      │  │   │
│   │   │   学习率:   lr = 1e-4                                                │  │   │
│   │   │   Betas:    (0.9, 0.95)                                              │  │   │
│   │   │   权重衰减: weight_decay = 0.01                                      │  │   │
│   │   │   Eps:      1e-15                                                    │  │   │
│   │   │                                                                      │  │   │
│   │   │   ┌─────────────────────────────────────────────────────────────┐    │  │   │
│   │   │   │                                                             │    │  │   │
│   │   │   │   m_t = β₁ × m_{t-1} + (1 - β₁) × g_t       # 一阶矩估计    │    │  │   │
│   │   │   │   v_t = β₂ × v_{t-1} + (1 - β₂) × g_t²      # 二阶矩估计    │    │  │   │
│   │   │   │                                                             │    │  │   │
│   │   │   │   m̂_t = m_t / (1 - β₁^t)                    # 偏差校正      │    │  │   │
│   │   │   │   v̂_t = v_t / (1 - β₂^t)                                    │    │  │   │
│   │   │   │                                                             │    │  │   │
│   │   │   │   θ_t = θ_{t-1} - lr × (m̂_t / (√v̂_t + ε) + wd × θ_{t-1})  │    │  │   │
│   │   │   │                                                             │    │  │   │
│   │   │   └─────────────────────────────────────────────────────────────┘    │  │   │
│   │   │                                                                      │  │   │
│   │   └──────────────────────────────────────────────────────────────────────┘  │   │
│   │                                                                             │   │
│   └─────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│   ┌─────────────────────────────────────────────────────────────────────────────┐   │
│   │                          5. EMA 更新                                         │   │
│   ├─────────────────────────────────────────────────────────────────────────────┤   │
│   │                                                                             │   │
│   │   ┌──────────────────────────────────────────────────────────────────────┐  │   │
│   │   │            Exponential Moving Average (EMA)                          │  │   │
│   │   │                                                                      │  │   │
│   │   │   EMA 参数用于推理，比训练参数更平滑稳定                             │  │   │
│   │   │                                                                      │  │   │
│   │   │   ┌─────────────────────────────────────────────────────────────┐    │  │   │
│   │   │   │                                                             │    │  │   │
│   │   │   │   ema_decay = 0.9999                                        │    │  │   │
│   │   │   │                                                             │    │  │   │
│   │   │   │   θ_ema = ema_decay × θ_ema + (1 - ema_decay) × θ           │    │  │   │
│   │   │   │                                                             │    │  │   │
│   │   │   │   即: θ_ema = 0.9999 × θ_ema + 0.0001 × θ                   │    │  │   │
│   │   │   │                                                             │    │  │   │
│   │   │   └─────────────────────────────────────────────────────────────┘    │  │   │
│   │   │                                                                      │  │   │
│   │   │   注: 只有 rank 0 维护 EMA 参数，节省内存                            │  │   │
│   │   │                                                                      │  │   │
│   │   └──────────────────────────────────────────────────────────────────────┘  │   │
│   │                                                                             │   │
│   └─────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

## 输入/输出规格

### 损失计算输入

| 输入 | 形状 | 描述 |
|-----|------|------|
| `v_pred` | `[B, N, 64]` | 模型预测的 velocity |
| `v_target` | `[B, N, 64]` | Flow Matching 目标 velocity |
| `loss_mask` | `[B, N]` | I2V 损失掩码 (可选) |

### 损失计算输出

| 输出 | 形状 | 描述 |
|-----|------|------|
| `loss` | `scalar` | 均方误差损失 |

## 关键代码

### 损失计算

```python
# 位置: opensora/utils/train.py

def get_batch_loss(
    pred: torch.Tensor,      # [B, N, C]
    target: torch.Tensor,    # [B, N, C]
    loss_mask: torch.Tensor = None,  # [B, N]
):
    """计算带掩码的 MSE 损失"""
    
    # 逐元素 MSE
    loss = (pred - target) ** 2  # [B, N, C]
    
    # 在通道维度求均值
    loss = loss.mean(dim=-1)  # [B, N]
    
    if loss_mask is not None:
        # 应用 I2V 掩码 (排除参考帧的损失)
        loss = loss * loss_mask
        # 归一化
        loss = loss.sum() / (loss_mask.sum() + 1e-6)
    else:
        loss = loss.mean()
    
    return loss
```

### 训练步骤

```python
# 位置: scripts/diffusion/train.py

def run_iter(model, batch, booster, optimizer, scheduler):
    """单次训练迭代"""
    
    # 1. 前向传播
    with torch.cuda.amp.autocast(enabled=True, dtype=torch.bfloat16):
        v_pred = model(
            img=batch["x_t"],
            img_ids=batch["img_ids"],
            txt=batch["txt"],
            txt_ids=batch["txt_ids"],
            timesteps=batch["timesteps"],
            y_vec=batch["y_vec"],
            guidance=batch["guidance"],
            cond=batch["cond"],
        )
        
        # 2. 损失计算
        loss = get_batch_loss(
            pred=v_pred,
            target=batch["v_target"],
            loss_mask=batch.get("loss_mask"),
        )
    
    # 3. 反向传播 (由 ColossalAI Booster 管理)
    booster.backward(loss, optimizer)
    
    # 4. 梯度裁剪
    grad_norm = optimizer.clip_grad_norm(max_norm=1.0)
    
    # 5. 参数更新
    optimizer.step()
    optimizer.zero_grad()
    
    # 6. 学习率调度
    scheduler.step()
    
    return loss.item(), grad_norm
```

### EMA 更新

```python
# 位置: opensora/utils/ema.py

class EMAModel:
    def __init__(self, model, decay=0.9999):
        self.decay = decay
        # 复制模型参数
        self.shadow = {}
        for name, param in model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()
    
    @torch.no_grad()
    def update(self, model):
        """更新 EMA 参数"""
        for name, param in model.named_parameters():
            if param.requires_grad:
                # θ_ema = decay * θ_ema + (1 - decay) * θ
                self.shadow[name].mul_(self.decay).add_(
                    param.data, alpha=1 - self.decay
                )
    
    def apply_shadow(self, model):
        """将 EMA 参数应用到模型 (推理时使用)"""
        for name, param in model.named_parameters():
            if name in self.shadow:
                param.data.copy_(self.shadow[name])
```

### ColossalAI 集成

```python
# 位置: scripts/diffusion/train.py

from colossalai.booster import Booster
from colossalai.booster.plugin import GeminiPlugin, HybridParallelPlugin

def setup_distributed():
    # 初始化分布式环境
    colossalai.launch_from_torch()
    
    # 选择并行策略
    if cfg.parallel == "zero2":
        plugin = GeminiPlugin(
            precision="bf16",
            initial_scale=2**16,
            max_norm=1.0,
        )
    elif cfg.parallel == "hybrid":
        plugin = HybridParallelPlugin(
            tp_size=cfg.tp_size,  # Tensor Parallelism
            pp_size=1,           # Pipeline Parallelism
            zero_stage=2,
            precision="bf16",
        )
    
    booster = Booster(plugin=plugin)
    return booster

def main():
    booster = setup_distributed()
    
    # 包装模型、优化器
    model, optimizer, _, _, scheduler = booster.boost(
        model=model,
        optimizer=optimizer,
        lr_scheduler=scheduler,
    )
    
    for epoch in range(num_epochs):
        for batch in dataloader:
            loss, grad_norm = run_iter(model, batch, booster, optimizer, scheduler)
            
            # EMA 更新 (只在 rank 0)
            if dist.get_rank() == 0:
                ema.update(model)
```

## I2V 损失掩码

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                              I2V 训练掩码机制                                        │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│   对于 Image-to-Video 任务，参考图像已知，不需要对其计算损失                          │
│                                                                                     │
│   ┌─────────────────────────────────────────────────────────────────────────────┐   │
│   │                                                                             │   │
│   │   视频序列:      [帧0] [帧1] [帧2] ... [帧16]                               │   │
│   │                    │                                                        │   │
│   │                    ▼                                                        │   │
│   │   I2V 模式:      [参考]                                                     │   │
│   │                    │                                                        │   │
│   │                    ▼                                                        │   │
│   │   loss_mask:     [ 0 ]  [ 1 ]  [ 1 ] ... [ 1  ]                            │   │
│   │                    │      │      │         │                                │   │
│   │                    ▼      ▼      ▼         ▼                                │   │
│   │   损失:          跳过   计算   计算  ...  计算                               │   │
│   │                                                                             │   │
│   └─────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│   代码实现:                                                                          │
│                                                                                     │
│   ```python                                                                         │
│   # 创建 loss_mask                                                                  │
│   num_frames = x_t.shape[2] // temporal_patch_size  # 17 // 1 = 17                 │
│   loss_mask = torch.ones(B, N)                                                     │
│                                                                                     │
│   if mode == "i2v":                                                                │
│       # 计算参考帧占用的 token 数                                                   │
│       tokens_per_frame = H_latent * W_latent // (2 * 2)  # patch 2x2               │
│       loss_mask[:, :tokens_per_frame] = 0  # 第一帧不计算损失                        │
│   ```                                                                               │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

## 学习率调度

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                              学习率调度策略                                          │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│   Warmup + Constant 策略:                                                           │
│                                                                                     │
│   lr ▲                                                                              │
│      │                    ┌─────────────────────────────────────                    │
│      │                   ╱                                                          │
│  1e-4│                  ╱                                                           │
│      │                 ╱                                                            │
│      │                ╱                                                             │
│      │               ╱                                                              │
│      │              ╱                                                               │
│      │             ╱                                                                │
│      │            ╱                                                                 │
│   0  │───────────┴──────────────────────────────────────────────▶ steps             │
│      0        warmup_steps                                                          │
│               (1000)                                                                │
│                                                                                     │
│   代码:                                                                              │
│   ```python                                                                         │
│   scheduler = get_scheduler(                                                        │
│       name="constant_with_warmup",                                                  │
│       optimizer=optimizer,                                                          │
│       num_warmup_steps=1000,                                                        │
│   )                                                                                 │
│   ```                                                                               │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

## 训练监控指标

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                              训练监控指标                                            │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│   日志记录 (TensorBoard / WandB):                                                   │
│                                                                                     │
│   ┌─────────────────────────────────────────────────────────────────────────────┐   │
│   │                                                                             │   │
│   │   train/loss:        MSE 损失值                                             │   │
│   │   train/grad_norm:   梯度范数 (裁剪前)                                       │   │
│   │   train/lr:          当前学习率                                             │   │
│   │   train/samples:     已处理样本数                                            │   │
│   │   train/throughput:  samples/second                                         │   │
│   │                                                                             │   │
│   │   memory/allocated:  GPU 已分配内存                                         │   │
│   │   memory/reserved:   GPU 保留内存                                           │   │
│   │                                                                             │   │
│   └─────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│   Checkpoint 保存:                                                                   │
│                                                                                     │
│   ┌─────────────────────────────────────────────────────────────────────────────┐   │
│   │                                                                             │   │
│   │   每 save_interval 步保存:                                                   │   │
│   │   - model_state_dict                                                        │   │
│   │   - optimizer_state_dict                                                    │   │
│   │   - scheduler_state_dict                                                    │   │
│   │   - ema_state_dict                                                          │   │
│   │   - step / epoch                                                            │   │
│   │                                                                             │   │
│   │   checkpoints/                                                              │   │
│   │   ├── step_10000/                                                           │   │
│   │   │   ├── model.pt                                                          │   │
│   │   │   ├── optimizer.pt                                                      │   │
│   │   │   ├── ema.pt                                                            │   │
│   │   │   └── meta.json                                                         │   │
│   │   ├── step_20000/                                                           │   │
│   │   └── latest/                                                               │   │
│   │                                                                             │   │
│   └─────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

## 完整训练配置示例

```yaml
# 训练配置
training:
  # 优化器
  optimizer:
    type: AdamW
    lr: 1e-4
    betas: [0.9, 0.95]
    weight_decay: 0.01
    eps: 1e-15
  
  # 学习率调度
  scheduler:
    type: constant_with_warmup
    warmup_steps: 1000
  
  # 梯度
  gradient:
    max_norm: 1.0
    accumulation_steps: 1
  
  # EMA
  ema:
    decay: 0.9999
    update_every: 1
  
  # 精度
  precision: bf16
  grad_checkpoint: true
  
  # 分布式
  parallel: zero2
  
  # 日志
  logging:
    log_interval: 10
    save_interval: 1000
    
# 训练循环
num_epochs: 100
batch_size: 16  # per GPU
global_batch_size: 256  # 16 GPUs × 16
```

## 内存优化技巧

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                              内存优化技术                                            │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│   1. Gradient Checkpointing                                                         │
│      - 前向: 只保存 block 边界激活                                                  │
│      - 反向: 重新计算中间激活                                                       │
│      - 节省: ~60% 激活内存                                                          │
│      - 代价: ~30% 额外计算                                                          │
│                                                                                     │
│   2. ZeRO-2 (梯度 + 优化器状态分片)                                                 │
│      - 梯度: 分片到各 GPU                                                           │
│      - 优化器状态: 分片 (m, v 各 1/N)                                               │
│      - 模型参数: 每 GPU 完整副本                                                    │
│      - 节省: ~8x 优化器内存                                                         │
│                                                                                     │
│   3. Mixed Precision (BF16)                                                         │
│      - 前向/反向: BF16                                                              │
│      - 参数主副本: FP32                                                             │
│      - 节省: ~50% 激活内存                                                          │
│                                                                                     │
│   4. Sequence Parallelism (可选)                                                    │
│      - 长序列分片到多 GPU                                                           │
│      - 适用于超长视频训练                                                            │
│                                                                                     │
│   内存估算 (8B 模型, 256px 17frames, batch=16):                                     │
│   ┌─────────────────────────────────────────────────────────────────────────────┐   │
│   │   组件              │  FP32         │  BF16 + ZeRO-2 + GradCkpt            │   │
│   │   ─────────────────────────────────────────────────────────────────────────│   │
│   │   模型参数          │  32 GB        │  16 GB                               │   │
│   │   梯度              │  32 GB        │  2 GB (分片)                         │   │
│   │   优化器状态        │  64 GB        │  8 GB (分片)                         │   │
│   │   激活              │  48 GB        │  8 GB (checkpointing)                │   │
│   │   ─────────────────────────────────────────────────────────────────────────│   │
│   │   总计              │  176 GB       │  34 GB                               │   │
│   │   单卡可行          │  ✗            │  ✓ (40GB A100)                       │   │
│   └─────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```
