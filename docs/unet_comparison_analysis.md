# UNet æ¶æ„å¯¹æ¯”åˆ†æï¼šç»å…¸ DDPM vs ä¸šç•Œå‰æ²¿

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£å¯¹æ¯”åˆ†ææ‚¨é¡¹ç›®ä¸­çš„ `ddpm.py` UNet å®ç°ä¸ä¸šç•Œä¸»æµçš„å‰æ²¿ UNet æ¶æ„ï¼ˆå¦‚ Stable Diffusionã€SDXLã€SD3ã€Flux ç­‰ï¼‰ï¼Œå¸®åŠ©æ‚¨ç†è§£å·®å¼‚å’Œæ”¹è¿›æ–¹å‘ã€‚

---

## 1. æ¶æ„æ€»è§ˆå¯¹æ¯”

| ç‰¹æ€§ | æ‚¨çš„å®ç° (DDPM UNet) | SD 1.x/2.x UNet | SDXL UNet | SD3/Flux (DiT) |
|------|---------------------|-----------------|-----------|----------------|
| **æ¶æ„ç±»å‹** | ç»å…¸ UNet | æ”¹è¿› UNet | å¤§è§„æ¨¡ UNet | DiT (Transformer) |
| **å‚æ•°é‡çº§** | ~10M | ~860M | ~2.6B | ~2-12B |
| **æ³¨æ„åŠ›ç±»å‹** | Self-Attention | Cross-Attention | Cross-Attention | Full Attention |
| **æ¡ä»¶æ³¨å…¥** | ä»… Timestep | Timestep + Text | Timestep + Text + Size | Timestep + Text |
| **å½’ä¸€åŒ–** | GroupNorm | GroupNorm | GroupNorm | AdaLN / RMSNorm |
| **ä½ç½®ç¼–ç ** | æ—  | æ—  | æ—  | RoPE |

---

## 2. è¯¦ç»†ç»„ä»¶å¯¹æ¯”

### 2.1 æ—¶é—´æ­¥åµŒå…¥ (Timestep Embedding)

#### æ‚¨çš„å®ç°
```python
# æ­£å¼¦ä½ç½®ç¼–ç  + 2å±‚ MLP
def timestep_embedding(timesteps, dim, max_period=10000):
    half = dim // 2
    freqs = torch.exp(-math.log(max_period) * torch.arange(...) / half)
    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)
    return embedding

# MLP æŠ•å½±
self.time_embed = nn.Sequential(
    nn.Linear(model_channels, time_embed_dim),
    nn.SiLU(),
    nn.Linear(time_embed_dim, time_embed_dim),
)
```

#### å‰æ²¿å®ç° (SD3/Flux é£æ ¼)
```python
class TimestepEmbedder(nn.Module):
    def __init__(self, hidden_size, frequency_embedding_size=256):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(frequency_embedding_size, hidden_size),
            nn.SiLU(),
            nn.Linear(hidden_size, hidden_size),
        )
        self.frequency_embedding_size = frequency_embedding_size

    def forward(self, t):
        # æ”¯æŒè¿ç»­æ—¶é—´æ­¥ (Flow Matching)
        t_freq = self.timestep_embedding(t, self.frequency_embedding_size)
        return self.mlp(t_freq)
```

**ä¸»è¦åŒºåˆ«ï¼š**
| æ–¹é¢ | æ‚¨çš„å®ç° | å‰æ²¿å®ç° |
|------|----------|----------|
| æ—¶é—´æ­¥èŒƒå›´ | ç¦»æ•£ [0, T] | è¿ç»­ [0, 1] (Flow Matching) |
| åµŒå…¥ç»´åº¦ | `model_channels` | ç‹¬ç«‹çš„ `frequency_embedding_size` |
| è°ƒåˆ¶æ–¹å¼ | åŠ æ³•æ³¨å…¥ | AdaLN è°ƒåˆ¶ (scale + shift) |

---

### 2.2 æ®‹å·®å— (Residual Block)

#### æ‚¨çš„å®ç°
```python
class ResidualBlock(TimestepBlock):
    def __init__(self, in_channels, out_channels, time_channels, dropout):
        self.conv1 = nn.Sequential(
            norm_layer(in_channels),  # GroupNorm
            nn.SiLU(),
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
        )
        # æ—¶é—´åµŒå…¥ï¼šç®€å•çš„åŠ æ³•
        self.time_emb = nn.Sequential(nn.SiLU(), nn.Linear(time_channels, out_channels))
        
        self.conv2 = nn.Sequential(
            norm_layer(out_channels),
            nn.SiLU(),
            nn.Dropout(p=dropout),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
        )

    def forward(self, x, t):
        h = self.conv1(x)
        h += self.time_emb(t)[:, :, None, None]  # ç®€å•åŠ æ³•
        h = self.conv2(h)
        return h + self.shortcut(x)
```

#### å‰æ²¿å®ç° (SDXL é£æ ¼)
```python
class ResnetBlock2D(nn.Module):
    def __init__(self, in_channels, out_channels, temb_channels, groups=32):
        self.norm1 = nn.GroupNorm(groups, in_channels, eps=1e-6)
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        
        # AdaGN: Adaptive Group Normalization
        self.time_emb_proj = nn.Linear(temb_channels, out_channels * 2)  # scale + shift
        
        self.norm2 = nn.GroupNorm(groups, out_channels, eps=1e-6)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        
    def forward(self, x, temb):
        h = self.norm1(x)
        h = F.silu(h)
        h = self.conv1(h)
        
        # AdaGN è°ƒåˆ¶
        temb_proj = self.time_emb_proj(F.silu(temb))
        scale, shift = temb_proj.chunk(2, dim=1)
        h = self.norm2(h) * (1 + scale[:, :, None, None]) + shift[:, :, None, None]
        
        h = F.silu(h)
        h = self.conv2(h)
        return h + self.shortcut(x)
```

**ä¸»è¦åŒºåˆ«ï¼š**
| æ–¹é¢ | æ‚¨çš„å®ç° | å‰æ²¿å®ç° |
|------|----------|----------|
| æ—¶é—´è°ƒåˆ¶ | åŠ æ³• (`h += t_emb`) | AdaGN (scale Ã— norm + shift) |
| è°ƒåˆ¶ä½ç½® | conv1 ä¹‹å | norm2 ä¹‹å |
| è¡¨è¾¾èƒ½åŠ› | è¾ƒå¼± | æ›´å¼ºçš„æ¡ä»¶æ§åˆ¶ |

---

### 2.3 æ³¨æ„åŠ›æœºåˆ¶ (Attention)

#### æ‚¨çš„å®ç°
```python
class AttentionBlock(nn.Module):
    def __init__(self, channels, num_heads=1):
        self.norm = norm_layer(channels)
        self.qkv = nn.Conv2d(channels, channels * 3, kernel_size=1, bias=False)
        self.proj = nn.Conv2d(channels, channels, kernel_size=1)

    def forward(self, x):
        B, C, H, W = x.shape
        qkv = self.qkv(self.norm(x))
        q, k, v = qkv.reshape(B * self.num_heads, -1, H * W).chunk(3, dim=1)
        
        # æ‰‹åŠ¨å®ç°æ³¨æ„åŠ›
        scale = 1.0 / math.sqrt(math.sqrt(C // self.num_heads))
        attn = torch.einsum("bct,bcs->bts", q * scale, k * scale)
        attn = attn.softmax(dim=-1)
        h = torch.einsum("bts,bcs->bct", attn, v)
        
        return self.proj(h.reshape(B, -1, H, W)) + x
```

#### å‰æ²¿å®ç° (SD3/Flux é£æ ¼)
```python
class Attention(nn.Module):
    def __init__(self, dim, num_heads=8, qk_norm=True):
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        
        # åˆ†ç¦»çš„ Q, K, V æŠ•å½±
        self.to_q = nn.Linear(dim, dim, bias=False)
        self.to_k = nn.Linear(dim, dim, bias=False)
        self.to_v = nn.Linear(dim, dim, bias=False)
        self.to_out = nn.Linear(dim, dim, bias=False)
        
        # QK Normalization (ç¨³å®šè®­ç»ƒ)
        self.q_norm = RMSNorm(self.head_dim) if qk_norm else nn.Identity()
        self.k_norm = RMSNorm(self.head_dim) if qk_norm else nn.Identity()

    def forward(self, x, context=None, freqs_cis=None):
        q = self.to_q(x)
        k = self.to_k(context if context is not None else x)
        v = self.to_v(context if context is not None else x)
        
        q = rearrange(q, 'b n (h d) -> b h n d', h=self.num_heads)
        k = rearrange(k, 'b n (h d) -> b h n d', h=self.num_heads)
        v = rearrange(v, 'b n (h d) -> b h n d', h=self.num_heads)
        
        # QK Norm
        q = self.q_norm(q)
        k = self.k_norm(k)
        
        # RoPE ä½ç½®ç¼–ç 
        if freqs_cis is not None:
            q = apply_rotary_emb(q, freqs_cis)
            k = apply_rotary_emb(k, freqs_cis)
        
        # Flash Attention
        out = F.scaled_dot_product_attention(q, k, v)
        
        out = rearrange(out, 'b h n d -> b n (h d)')
        return self.to_out(out)
```

**ä¸»è¦åŒºåˆ«ï¼š**
| æ–¹é¢ | æ‚¨çš„å®ç° | å‰æ²¿å®ç° |
|------|----------|----------|
| æ³¨æ„åŠ›ç±»å‹ | ä»… Self-Attention | Self + Cross Attention |
| QKV æŠ•å½± | åˆå¹¶çš„ Conv2d | åˆ†ç¦»çš„ Linear |
| ä½ç½®ç¼–ç  | æ—  | RoPE (æ—‹è½¬ä½ç½®ç¼–ç ) |
| QK å½’ä¸€åŒ– | æ—  | RMSNorm (ç¨³å®šå¤§æ¨¡å‹è®­ç»ƒ) |
| è®¡ç®—ä¼˜åŒ– | æ‰‹åŠ¨ einsum | Flash Attention / SDPA |
| æ¡ä»¶æ³¨å…¥ | æ—  | æ–‡æœ¬ context ä½œä¸º KV |

---

### 2.4 æ¡ä»¶æ³¨å…¥æœºåˆ¶

#### æ‚¨çš„å®ç°
```
ä»…æ”¯æŒ Timestep æ¡ä»¶ï¼š
timestep â†’ sinusoidal embedding â†’ MLP â†’ åŠ åˆ° ResBlock
```

#### å‰æ²¿å®ç° (å¤šæ¡ä»¶)
```
1. Timestep æ¡ä»¶:
   t â†’ sinusoidal â†’ MLP â†’ AdaLN è°ƒåˆ¶

2. æ–‡æœ¬æ¡ä»¶ (Cross-Attention):
   text â†’ CLIP/T5 Encoder â†’ Cross-Attention KV

3. å›¾åƒæ¡ä»¶ (IP-Adapter é£æ ¼):
   image â†’ Image Encoder â†’ ä¸ KV concat

4. é¢å¤–æ¡ä»¶ (SDXL):
   - original_size, crop_coords â†’ åµŒå…¥
   - micro_conditioning â†’ pooled text embed æ‹¼æ¥
```

---

### 2.5 æ¶æ„å¸ƒå±€

#### æ‚¨çš„å®ç°
```
channel_mult=(1, 2, 2, 2)  â†’  128 â†’ 256 â†’ 256 â†’ 256
num_res_blocks=2
attention_resolutions=(8, 16)  â†’  ä»…åœ¨ä½åˆ†è¾¨ç‡æ·»åŠ æ³¨æ„åŠ›
```

#### å‰æ²¿å®ç° (SDXL)
```python
# SDXL UNet é…ç½®
block_out_channels = (320, 640, 1280)
layers_per_block = 2
transformer_layers_per_block = [1, 2, 10]  # æ›´æ·±çš„ Transformer
attention_head_dim = [5, 10, 20]
cross_attention_dim = 2048  # æ›´å¤§çš„æ–‡æœ¬åµŒå…¥

# SD3/Flux (çº¯ Transformer)
hidden_size = 3072
num_layers = 24  # 24 ä¸ª DiT blocks
num_attention_heads = 24
```

---

## 3. å…³é”®æŠ€æœ¯å·®è·

### 3.1 ç¼ºå¤±çš„æ ¸å¿ƒåŠŸèƒ½

| åŠŸèƒ½ | æ‚¨çš„å®ç° | å‰æ²¿å®ç° | é‡è¦æ€§ |
|------|----------|----------|--------|
| **Cross-Attention** | âŒ | âœ… | ğŸ”´ å…³é”® - æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ ¸å¿ƒ |
| **AdaLN è°ƒåˆ¶** | âŒ (ç®€å•åŠ æ³•) | âœ… | ğŸ”´ å…³é”® - æ›´å¥½çš„æ¡ä»¶æ§åˆ¶ |
| **QK Normalization** | âŒ | âœ… | ğŸŸ¡ é‡è¦ - å¤§æ¨¡å‹è®­ç»ƒç¨³å®šæ€§ |
| **RoPE ä½ç½®ç¼–ç ** | âŒ | âœ… | ğŸŸ¡ é‡è¦ - ä»»æ„åˆ†è¾¨ç‡ç”Ÿæˆ |
| **Flash Attention** | âŒ | âœ… | ğŸŸ¢ ä¼˜åŒ– - æ˜¾å­˜å’Œé€Ÿåº¦ |
| **VAE æ½œç©ºé—´** | âŒ (åƒç´ ç©ºé—´) | âœ… | ğŸ”´ å…³é”® - é«˜åˆ†è¾¨ç‡ç”Ÿæˆ |
| **å¤šå°ºåº¦æ—¶é—´åµŒå…¥** | âŒ | âœ… | ğŸŸ¡ é‡è¦ - æ›´å¥½çš„è°ƒåº¦ |

### 3.2 æ¶æ„æ¼”è¿›è¶‹åŠ¿

```
DDPM UNet (2020)          â†’  æ‚¨çš„å®ç°
    â†“
Stable Diffusion 1.x (2022) â†’  æ·»åŠ  Cross-Attention, VAE
    â†“
Stable Diffusion 2.x (2022) â†’  æ›´å¤§çš„ text encoder
    â†“
SDXL (2023)               â†’  æ›´å¤§çš„ UNet, åŒ text encoder
    â†“
SD3 / Flux (2024)         â†’  MM-DiT (çº¯ Transformer)
    â†“
Z-Image (2025)            â†’  ä¼˜åŒ–çš„ DiT + Turbo è’¸é¦
```

---

## 4. æ”¹è¿›å»ºè®®

### 4.1 çŸ­æœŸæ”¹è¿› (ä¿æŒ UNet æ¶æ„)

```python
# 1. æ·»åŠ  AdaLN è°ƒåˆ¶
class ImprovedResidualBlock(nn.Module):
    def forward(self, x, temb):
        h = self.norm1(x)
        h = F.silu(h)
        h = self.conv1(h)
        
        # AdaLN æ›¿ä»£ç®€å•åŠ æ³•
        scale, shift = self.time_emb(F.silu(temb)).chunk(2, dim=1)
        h = self.norm2(h) * (1 + scale[..., None, None]) + shift[..., None, None]
        
        h = F.silu(h)
        h = self.conv2(h)
        return h + self.shortcut(x)

# 2. æ·»åŠ  Cross-Attention
class CrossAttention(nn.Module):
    def __init__(self, query_dim, context_dim):
        self.to_q = nn.Linear(query_dim, query_dim)
        self.to_k = nn.Linear(context_dim, query_dim)
        self.to_v = nn.Linear(context_dim, query_dim)
        
    def forward(self, x, context):
        q = self.to_q(x)
        k = self.to_k(context)
        v = self.to_v(context)
        return F.scaled_dot_product_attention(q, k, v)

# 3. ä½¿ç”¨ Flash Attention
# æ›¿æ¢æ‰‹åŠ¨ einsum ä¸º:
out = F.scaled_dot_product_attention(q, k, v)
```

### 4.2 ä¸­æœŸæ”¹è¿› (è¿ç§»åˆ° Latent Diffusion)

```python
# 1. æ·»åŠ  VAE
class AutoencoderKL:
    def encode(self, x):
        # å›¾åƒ â†’ æ½œç©ºé—´ (8x ä¸‹é‡‡æ ·)
        return self.encoder(x)
    
    def decode(self, z):
        # æ½œç©ºé—´ â†’ å›¾åƒ
        return self.decoder(z)

# 2. è®­ç»ƒæµç¨‹æ”¹å˜
# åŸå§‹: noise_pred = model(x_noisy, t)
# æ”¹è¿›: noise_pred = model(z_noisy, t, text_embed)
```

### 4.3 é•¿æœŸæ¼”è¿› (DiT æ¶æ„)

å‚è€ƒ Z-Image çš„å®ç°ï¼š
- çº¯ Transformer æ¶æ„
- Patchify + ä½ç½®ç¼–ç 
- åŒå‘æ³¨æ„åŠ› (å›¾åƒ + æ–‡æœ¬)
- Flow Matching è®­ç»ƒ

---

## 5. ä»£ç ç»“æ„å¯¹æ¯”å›¾

### æ‚¨çš„ UNet
```
Input (x, t)
    â”‚
    â”œâ”€â–º Timestep Embedding â”€â–º time_emb
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Down Blocks             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ ResBlock + time_emb add â”‚    â”‚
â”‚  â”‚ [Optional] Self-Attn    â”‚    â”‚
â”‚  â”‚ Downsample              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚ (skip connections)
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Middle Block             â”‚
â”‚  ResBlock â†’ Self-Attn â†’ ResBlockâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Up Blocks              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Concat skip connection  â”‚    â”‚
â”‚  â”‚ ResBlock + time_emb add â”‚    â”‚
â”‚  â”‚ [Optional] Self-Attn    â”‚    â”‚
â”‚  â”‚ Upsample                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
Output (noise prediction)
```

### SD3/Flux DiT
```
Input (x, t, text)
    â”‚
    â”œâ”€â–º Timestep Embedding â”€â–º adaln_input
    â”œâ”€â–º Text Encoder â”€â”€â”€â”€â”€â”€â”€â”€â–º text_embeds
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Patchify + Embed          â”‚
â”‚  x: [B,C,H,W] â†’ [B, N, D]       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    DiT Blocks (Ã— N layers)      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ AdaLN(x, adaln_input)   â”‚    â”‚
â”‚  â”‚ Self-Attention + RoPE   â”‚    â”‚
â”‚  â”‚ Cross-Attention(x, text)â”‚    â”‚
â”‚  â”‚ AdaLN + FFN             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Final Layer               â”‚
â”‚  AdaLN â†’ Linear â†’ Unpatchify    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
Output (velocity / noise)
```

---

## 6. æ€»ç»“

æ‚¨çš„å®ç°æ˜¯ä¸€ä¸ª**ç»å…¸çš„ DDPM UNet**ï¼Œé€‚åˆå­¦ä¹ å’Œç†è§£æ‰©æ•£æ¨¡å‹çš„åŸºæœ¬åŸç†ã€‚ä¸ä¸šç•Œå‰æ²¿ç›¸æ¯”ï¼Œä¸»è¦å·®è·åœ¨äºï¼š

1. **æ¡ä»¶æœºåˆ¶**: ç¼ºå°‘ Cross-Attentionï¼Œæ— æ³•è¿›è¡Œæ–‡æœ¬å¼•å¯¼ç”Ÿæˆ
2. **è°ƒåˆ¶æ–¹å¼**: ä½¿ç”¨ç®€å•åŠ æ³•è€Œé AdaLNï¼Œæ¡ä»¶æ§åˆ¶èƒ½åŠ›è¾ƒå¼±
3. **è§„æ¨¡å·®å¼‚**: å‚æ•°é‡çº§å·®è· (~10M vs ~2B+)
4. **è®­ç»ƒèŒƒå¼**: åƒç´ ç©ºé—´ DDPM vs æ½œç©ºé—´ Flow Matching
5. **ä¼˜åŒ–æŠ€æœ¯**: ç¼ºå°‘ Flash Attentionã€QK Norm ç­‰ç°ä»£ä¼˜åŒ–

å»ºè®®æ ¹æ®æ‚¨çš„ç›®æ ‡ï¼š
- **å­¦ä¹ ç›®çš„**: å½“å‰å®ç°è¶³å¤Ÿç†è§£æ ¸å¿ƒæ¦‚å¿µ
- **å®é™…åº”ç”¨**: è€ƒè™‘ä½¿ç”¨ diffusers åº“çš„é¢„è®­ç»ƒæ¨¡å‹
- **æ·±å…¥ç ”ç©¶**: å‚è€ƒ SD3/Flux çš„ DiT æ¶æ„è¿›è¡Œæ”¹è¿›
