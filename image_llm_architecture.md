================================================================================
                    图像生成模型架构示意图 (类LLM方式)
                    Patch大小: 2x2 | 离散值步长: 0.25
================================================================================

【配置参数】
  - 图像尺寸: 32x32 像素
  - Patch大小: 2x2 像素
  - 通道数: 1 (灰度图)
  - 像素离散值: {0.00, 0.25, 0.50, 0.75, 1.00} 共5个值
  - Patch数量: (32/2) × (32/2) = 16 × 16 = 256 个Patch


================================================================================
【1. 图像分割为Patch】
================================================================================

  32x32 原始图像 (单通道灰度图)
  ┌──────────────────────────────────────────────────────────────────────────┐
  │                                                                          │
  │    ← ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  32 像素  ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ →           │
  │                                                                          │
  │  ↑ ┌────┬────┬────┬────┬────┬────┬────┬────┬─ ─ ─ ┬────┬────┐          │
  │  │ │ P0 │ P1 │ P2 │ P3 │ P4 │ P5 │ P6 │ P7 │ ...  │P14 │P15 │  ← 第0行  │
  │  │ ├────┼────┼────┼────┼────┼────┼────┼────┼─ ─ ─ ┼────┼────┤          │
  │  │ │P16 │P17 │P18 │P19 │P20 │P21 │P22 │P23 │ ...  │P30 │P31 │  ← 第1行  │
  │  │ ├────┼────┼────┼────┼────┼────┼────┼────┼─ ─ ─ ┼────┼────┤          │
  │  32 │P32 │P33 │    │    │    │    │    │    │      │    │P47 │  ← 第2行  │
  │ 像 ├────┼────┼────┼────┼────┼────┼────┼────┼─ ─ ─ ┼────┼────┤          │
  │ 素 │    │    │    │    │    │    │    │    │      │    │    │          │
  │  │ ├────┼────┼────┼────┼────┼────┼────┼────┼─ ─ ─ ┼────┼────┤          │
  │  │ │    │    │    │    │    │    │    │    │      │    │    │          │
  │  │ ├────┼────┼────┼────┼────┼────┼────┼────┼─ ─ ─ ┼────┼────┤          │
  │  │ │    │    │    │    │    │    │    │    │  ... │    │    │          │
  │  │ ├────┼────┼────┼────┼────┼────┼────┼────┼─ ─ ─ ┼────┼────┤          │
  │  ↓ │P240│P241│P242│P243│P244│P245│P246│P247│ ...  │P254│P255│  ← 第15行 │
  │    └────┴────┴────┴────┴────┴────┴────┴────┴─ ─ ─ ┴────┴────┘          │
  │                                                                          │
  │    每行 16 个 Patch，共 16 行，总计 256 个 Patch                          │
  │                                                                          │
  └──────────────────────────────────────────────────────────────────────────┘


  单个Patch结构 (2×2 = 4个像素):
  ┌─────────────┐
  │  px0 │ px1  │     每个像素值 ∈ {0.00, 0.25, 0.50, 0.75, 1.00}
  ├─────────────┤     
  │  px2 │ px3  │     共5种可能的取值
  └─────────────┘


================================================================================
【2. Patch量化为Token (词表构建)】
================================================================================

  每个Patch有4个像素，每个像素有5种取值
  
  词表大小 = 5^4 = 625 种可能的Patch组合

  词表示例:
  ┌─────────┬─────────────────────────────┬────────────────────────┐
  │ Token ID│     Patch像素值组合          │      可视化示意        │
  ├─────────┼─────────────────────────────┼────────────────────────┤
  │    0    │ [0.00, 0.00, 0.00, 0.00]    │  ░░    (全黑)          │
  │         │                             │  ░░                    │
  ├─────────┼─────────────────────────────┼────────────────────────┤
  │    1    │ [0.00, 0.00, 0.00, 0.25]    │  ░░                    │
  │         │                             │  ░▒                    │
  ├─────────┼─────────────────────────────┼────────────────────────┤
  │    2    │ [0.00, 0.00, 0.00, 0.50]    │  ░░                    │
  │         │                             │  ░▓                    │
  ├─────────┼─────────────────────────────┼────────────────────────┤
  │   ...   │          ...                │         ...            │
  ├─────────┼─────────────────────────────┼────────────────────────┤
  │   312   │ [0.50, 0.50, 0.50, 0.50]    │  ▓▓    (中灰)          │
  │         │                             │  ▓▓                    │
  ├─────────┼─────────────────────────────┼────────────────────────┤
  │   ...   │          ...                │         ...            │
  ├─────────┼─────────────────────────────┼────────────────────────┤
  │   624   │ [1.00, 1.00, 1.00, 1.00]    │  ██    (全白)          │
  │         │                             │  ██                    │
  └─────────┴─────────────────────────────┴────────────────────────┘

  Token编码公式 (从像素值到Token ID):
  ┌────────────────────────────────────────────────────────────────┐
  │                                                                │
  │  pixel_idx = pixel_value / 0.25  →  {0, 1, 2, 3, 4}           │
  │                                                                │
  │  token_id = px0_idx * 5³ + px1_idx * 5² + px2_idx * 5¹        │
  │           + px3_idx * 5⁰                                       │
  │                                                                │
  │  例如: [0.25, 0.50, 0.75, 1.00]                                │
  │        → [1, 2, 3, 4]                                          │
  │        → 1×125 + 2×25 + 3×5 + 4×1 = 125 + 50 + 15 + 4 = 194   │
  │                                                                │
  └────────────────────────────────────────────────────────────────┘


================================================================================
【3. 图像到Token序列的转换】
================================================================================

  原始图像 32×32
       ↓
  分割成 16×16 = 256 个 Patch
       ↓
  按光栅扫描顺序展开为序列
       ↓
  每个Patch量化为Token ID

  ┌─────────────────────────────────────────────────────────────────────┐
  │                                                                     │
  │  图像Patch排列:                                                      │
  │                                                                     │
  │     P0   P1   P2   P3  ... P15                                      │
  │     P16  P17  P18  P19 ... P31                                      │
  │     P32  ...                                                        │
  │     ...                                                             │
  │     P240 P241 ...          P255                                     │
  │                                                                     │
  │                     ↓  光栅扫描                                      │
  │                                                                     │
  │  Token序列:                                                         │
  │                                                                     │
  │  ┌─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┐     │
  │  │[BOS]│ T0  │ T1  │ T2  │ T3  │ ... │T254 │T255 │[EOS]│     │     │
  │  └─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┘     │
  │    ↑                                             ↑                  │
  │  起始符                                        结束符                │
  │                                                                     │
  │  序列总长度: 1 (BOS) + 256 (Patches) + 1 (EOS) = 258                │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘


================================================================================
【4. 自回归生成流程 (类LLM)】
================================================================================

  ┌─────────────────────────────────────────────────────────────────────┐
  │                                                                     │
  │  Step 0:  输入: [BOS]                                               │
  │           ┌─────┐                                                   │
  │           │[BOS]│ ──→ Model ──→ P(T₀|BOS) ──→ Sample ──→ T₀        │
  │           └─────┘                   ↓                               │
  │                              ┌──────────────┐                       │
  │                              │ 0: 0.02      │                       │
  │                              │ 1: 0.01      │                       │
  │                              │ ...          │ 625个概率值            │
  │                              │ 312: 0.15 ← │ 采样得到312            │
  │                              │ ...          │                       │
  │                              │ 624: 0.03    │                       │
  │                              └──────────────┘                       │
  │                                                                     │
  │  Step 1:  输入: [BOS, T₀]                                           │
  │           ┌─────┬─────┐                                             │
  │           │[BOS]│ T₀  │ ──→ Model ──→ P(T₁|BOS,T₀) ──→ Sample T₁   │
  │           └─────┴─────┘                                             │
  │                                                                     │
  │  Step 2:  输入: [BOS, T₀, T₁]                                       │
  │           ┌─────┬─────┬─────┐                                       │
  │           │[BOS]│ T₀  │ T₁  │ ──→ Model ──→ P(T₂|...) ──→ Sample T₂│
  │           └─────┴─────┴─────┘                                       │
  │                                                                     │
  │            ...重复256次...                                          │
  │                                                                     │
  │  Step 255: 输入: [BOS, T₀, T₁, ..., T₂₅₄]                           │
  │            ──→ Model ──→ P(T₂₅₅|...) ──→ Sample T₂₅₅               │
  │                                                                     │
  │  最终序列: [T₀, T₁, T₂, ..., T₂₅₅]                                  │
  │            ↓                                                        │
  │  反量化重组为 32×32 图像                                             │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘


================================================================================
【5. 模型架构 (Transformer Decoder)】
================================================================================

  ┌─────────────────────────────────────────────────────────────────────┐
  │                                                                     │
  │   输入: Token IDs序列 [BOS, T₀, T₁, ..., T_{n-1}]                   │
  │                                                                     │
  │         ↓                                                           │
  │   ┌─────────────────────────────────────────────────────────────┐  │
  │   │                                                             │  │
  │   │   Token Embedding Layer                                     │  │
  │   │   ┌───────────────────────────────────────────────────┐    │  │
  │   │   │  vocab_size = 625 + 2 = 627                       │    │  │
  │   │   │  (625个Patch token + BOS + EOS)                   │    │  │
  │   │   │                                                   │    │  │
  │   │   │  embedding_dim = d_model (例如: 256, 512)         │    │  │
  │   │   └───────────────────────────────────────────────────┘    │  │
  │   │                                                             │  │
  │   └─────────────────────────────────────────────────────────────┘  │
  │         ↓                                                           │
  │   ┌─────────────────────────────────────────────────────────────┐  │
  │   │                                                             │  │
  │   │   Transformer Decoder Block × N层 (例如: 6, 12层)           │  │
  │   │                                                             │  │
  │   │   ┌───────────────────────────────────────────────────┐    │  │
  │   │   │                                                   │    │  │
  │   │   │   ┌─────────────────────────────────────────┐    │    │  │
  │   │   │   │  Masked Multi-Head Self-Attention + 2D RoPE │   │    │  │
  │   │   │   │                                         │    │    │  │
  │   │   │   │  Q ──→ Apply 2D RoPE ──┐                │    │    │  │
  │   │   │   │  K ──→ Apply 2D RoPE ──┼──→ Attention ──→ Out │   │  │
  │   │   │   │  V ────────────────────┘      ↑         │    │    │  │
  │   │   │   │                               │         │    │    │  │
  │   │   │   │      Causal Mask (下三角矩阵)           │    │    │  │
  │   │   │   │      只能看到当前及之前的token          │    │    │  │
  │   │   │   └─────────────────────────────────────────┘    │    │  │
  │   │   │                      ↓                           │    │  │
  │   │   │              Add & LayerNorm                     │    │  │
  │   │   │                      ↓                           │    │  │
  │   │   │   ┌─────────────────────────────────────────┐    │    │  │
  │   │   │   │  Feed Forward Network (FFN)             │    │    │  │
  │   │   │   │                                         │    │    │  │
  │   │   │   │  Linear(d_model → 4*d_model)            │    │    │  │
  │   │   │   │       ↓                                 │    │    │  │
  │   │   │   │  GELU / ReLU                            │    │    │  │
  │   │   │   │       ↓                                 │    │    │  │
  │   │   │   │  Linear(4*d_model → d_model)            │    │    │  │
  │   │   │   └─────────────────────────────────────────┘    │    │  │
  │   │   │                      ↓                           │    │  │
  │   |   │              Add & LayerNorm                     │    │  │
  └─────────────────────────────────────────────────────────────────────┘


================================================================================
【6. 训练流程】
================================================================================

  ┌─────────────────────────────────────────────────────────────────────┐
  │                                                                     │
  │  训练数据准备:                                                       │
  │                                                                     │
  │  原始图像 ──→ 分割Patch ──→ 量化Token ──→ Token序列                  │
  │     ↓                                                               │
  │  ┌─────────────────────────────────────────────────────────────┐   │
  │  │ 图像1: [BOS, 312, 156, 78, 400, ..., 234, EOS]              │   │
  │  │ 图像2: [BOS, 0, 0, 125, 250, ..., 624, EOS]                 │   │
  │  │ 图像3: [BOS, 100, 200, 300, 400, ..., 500, EOS]             │   │
  │  │ ...                                                         │   │
  │  └─────────────────────────────────────────────────────────────┘   │
  │                                                                     │
  │  Teacher Forcing 训练:                                              │
  │                                                                     │
  │  Input序列:   [BOS,  T₀,   T₁,   T₂,  ..., T₂₅₄, T₂₅₅]            │
  │                 ↓     ↓     ↓     ↓          ↓     ↓               │
  │               Model Model Model Model     Model  Model             │
  │                 ↓     ↓     ↓     ↓          ↓     ↓               │
  │  Prediction:  [P₀,   P₁,   P₂,   P₃,  ..., P₂₅₅, P_eos]           │
  │                 ↓     ↓     ↓     ↓          ↓     ↓               │
  │  Target:      [T₀,   T₁,   T₂,   T₃,  ..., T₂₅₅, EOS ]            │
  │                 ↓     ↓     ↓     ↓          ↓     ↓               │
  │              CrossEntropyLoss (每个位置)                            │
  │                 ↓     ↓     ↓     ↓          ↓     ↓               │
  │                 └─────┴─────┴─────┴──────────┴─────┘               │
  │                              ↓                                      │
  │                    Total Loss = Mean(all positions)                │
  │                              ↓                                      │
  │                        Backpropagation                             │
  │                              ↓                                      │
  │                       Update Parameters                            │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘


================================================================================
【7. 推理/生成流程】
================================================================================

  ┌─────────────────────────────────────────────────────────────────────┐
  │                                                                     │
  │  自回归生成 (Autoregressive Generation):                            │
  │                                                                     │
  │  generated_tokens = [BOS]                                           │
  │                                                                     │
  │  for step in range(256):  # 生成256个Patch Token                    │
  │      │                                                              │
  │      ├──→ logits = model(generated_tokens)                         │
  │      │         ↓                                                    │
  │      ├──→ next_token_logits = logits[:, -1, :]  # 最后位置          │
  │      │         ↓                                                    │
  │      ├──→ probs = softmax(next_token_logits / temperature)         │
  │      │         ↓                                                    │
  │      │    ┌─────────────────────────────────────────┐              │
  │      │    │ 采样策略:                               │              │
  │      │    │  - Greedy: argmax(probs)               │              │
  │      │    │  - Random: sample from probs           │              │
  │      │    │  - Top-k: 从top-k中采样                 │              │
  │      │    │  - Top-p (Nucleus): 累积概率截断        │              │
  │      │    └─────────────────────────────────────────┘              │
  │      │         ↓                                                    │
  │      └──→ generated_tokens.append(next_token)                      │
  │                                                                     │
  │  end for                                                            │
  │                                                                     │
  │  output_tokens = generated_tokens[1:]  # 去掉BOS                    │
  │        ↓                                                            │
  │  ┌─────────────────────────────────────────────────────────────┐   │
  │  │  Token反量化:                                               │   │
  │  │                                                             │   │
  │  │  token_id = 194                                             │   │
  │  │       ↓                                                     │   │
  │  │  px3 = (194 % 5) * 0.25 = 4 * 0.25 = 1.00                  │   │
  │  │  px2 = ((194 // 5) % 5) * 0.25 = 3 * 0.25 = 0.75           │   │
  │  │  px1 = ((194 // 25) % 5) * 0.25 = 2 * 0.25 = 0.50          │   │
  │  │  px0 = (194 // 125) * 0.25 = 1 * 0.25 = 0.25               │   │
  │  │       ↓                                                     │   │
  │  │  patch_pixels = [0.25, 0.50, 0.75, 1.00]                    │   │
  │  │                                                             │   │
  │  └─────────────────────────────────────────────────────────────┘   │
  │        ↓                                                            │
  │  重组为 16×16 Patch网格                                             │
  │        ↓                                                            │
  │  展开为 32×32 像素图像                                              │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘


================================================================================
【8. 数据流总览】
================================================================================

  ┌─────────────────────────────────────────────────────────────────────┐
  │                                                                     │
  │                         ┌─────────────┐                            │
  │                         │  32×32图像   │                            │
  │                         │  (灰度)      │                            │
  │                         └──────┬──────┘                            │
  │                                │                                    │
  │                    ┌───────────┴───────────┐                       │
  │                    ▼                       ▼                        │
  │            ┌──────────────┐        ┌──────────────┐                │
  │            │  训练阶段     │        │  推理阶段     │                │
  │            └──────┬───────┘        └──────┬───────┘                │
  │                   │                       │                         │
  │                   ▼                       ▼                         │
  │          ┌────────────────┐      ┌────────────────┐                │
  │          │ 分割成2×2 Patch │      │   从[BOS]开始  │                │
  │          │    (256个)     │      │                │                │
  │          └───────┬────────┘      └───────┬────────┘                │
  │                  │                       │                         │
  │                  ▼                       ▼                         │
  │          ┌────────────────┐      ┌────────────────┐                │
  │          │ 量化为Token ID │      │ 自回归生成     │                │
  │          │  (词表:625)    │      │ 256个Token    │                │
  │          └───────┬────────┘      └───────┬────────┘                │
  │                  │                       │                         │
  │                  ▼                       ▼                         │
  │          ┌────────────────┐      ┌────────────────┐                │
  │          │ 组成序列       │      │ 反量化Token    │                │
  │          │ [BOS,T0..T255] │      │ 得到Patch像素  │                │
  │          └───────┬────────┘      └───────┬────────┘                │
  │                  │                       │                         │
  │                  ▼                       ▼                         │
  │          ┌────────────────┐      ┌────────────────┐                │
  │          │ Transformer    │      │ 重组为         │                │
  │          │ 前向传播+Loss  │      │ 32×32图像      │                │
  │          └───────┬────────┘      └───────┬────────┘                │
  │                  │                       │                         │
  │                  ▼                       ▼                         │
  │          ┌────────────────┐      ┌────────────────┐                │
  │          │ 反向传播       │      │  输出生成图像   │                │
  │          │ 更新参数       │      │                │                │
  │          └────────────────┘      └────────────────┘                │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘


================================================================================
【9. 关键参数汇总】
================================================================================

  ┌─────────────────────────────────────────────────────────────────────┐
  │                                                                     │
  │  图像参数:                                                          │
  │  ├── image_size      = 32        # 图像尺寸                         │
  │  ├── patch_size      = 2         # Patch尺寸                        │
  │  ├── num_patches     = 256       # Patch数量 (16×16)                │
  │  ├── pixels_per_patch = 4        # 每个Patch的像素数 (2×2)          │
  │  └── channels        = 1         # 通道数                           │
  │                                                                     │
  │  量化参数:                                                          │
  │  ├── value_step      = 0.25      # 离散值步长                       │
  │  ├── num_values      = 5         # 每个像素的可能取值数              │
  │  │                               # (0.00, 0.25, 0.50, 0.75, 1.00)   │
  │  └── vocab_size      = 627       # 词表大小 (5^4 + BOS + EOS)       │
  │                                                                     │
  │  模型参数 (示例配置):                                                │
  │  ├── d_model         = 256       # 隐藏层维度                       │
  │  ├── n_heads         = 8         # 注意力头数                       │
  │  ├── n_layers        = 6         # Transformer层数                  │
  │  ├── d_ff            = 1024      # FFN中间层维度 (4×d_model)        │
  │  ├── max_seq_len     = 258       # 最大序列长度 (1+256+1)           │
  │  └── dropout         = 0.1       # Dropout率                        │
  │                                                                     │
  │  训练参数 (示例配置):                                                │
  │  ├── batch_size      = 64                                          │
  │  ├── learning_rate   = 1e-4                                        │
  │  ├── epochs          = 100                                         │
  │  └── optimizer       = AdamW                                       │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘


================================================================================
【10. 2D旋转位置编码 (2D RoPE) 详解】
================================================================================

  ┌─────────────────────────────────────────────────────────────────────┐
  │                                                                     │
  │  【原理】                                                           │
  │  RoPE (Rotary Position Embedding) 通过旋转矩阵将位置信息编码到      │
  │  Query和Key向量中。2D RoPE扩展到二维空间，分别对行和列位置编码。    │
  │                                                                     │
  │  【Patch的2D坐标】                                                  │
  │                                                                     │
  │  对于第i个Patch (光栅扫描顺序):                                     │
  │    row = i // 16    (行坐标: 0~15)                                 │
  │    col = i % 16     (列坐标: 0~15)                                 │
  │                                                                     │
  │  例如:                                                              │
  │    P0  → (row=0, col=0)                                            │
  │    P1  → (row=0, col=1)                                            │
  │    P16 → (row=1, col=0)                                            │
  │    P255→ (row=15,col=15)                                           │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘

  ┌─────────────────────────────────────────────────────────────────────┐
  │                                                                     │
  │  【2D RoPE 维度分配】                                               │
  │                                                                     │
  │  假设 head_dim = 64 (每个注意力头的维度)                            │
  │                                                                     │
  │  将维度均分给行和列:                                                │
  │    - 前 32 维: 用于编码 行位置 (row)                                │
  │    - 后 32 维: 用于编码 列位置 (col)                                │
  │                                                                     │
  │  ┌────────────────────────────────────────────────────────────┐    │
  │  │     head_dim = 64                                          │    │
  │  │  ┌─────────────────────┬─────────────────────┐            │    │
  │  │  │   Row RoPE (32维)   │   Col RoPE (32维)   │            │    │
  │  │  │   编码行位置 row     │   编码列位置 col    │            │    │
  │  │  └─────────────────────┴─────────────────────┘            │    │
  │  └────────────────────────────────────────────────────────────┘    │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘

  ┌─────────────────────────────────────────────────────────────────────┐
  │                                                                     │
  │  【旋转编码公式】                                                   │
  │                                                                     │
  │  对于维度对 (d_{2k}, d_{2k+1}), 位置为 m 时:                        │
  │                                                                     │
  │  θ_k = 10000^(-2k/dim)   (频率基数)                                │
  │                                                                     │
  │  旋转角度: m × θ_k                                                  │
  │                                                                     │
  │  ┌                    ┐   ┌                        ┐   ┌      ┐    │
  │  │ q'_{2k}            │   │ cos(m·θ_k)  -sin(m·θ_k) │   │ q_{2k}│   │
  │  │                    │ = │                        │ × │      │    │
  │  │ q'_{2k+1}          │   │ sin(m·θ_k)   cos(m·θ_k) │   │q_{2k+1}   │
  │  └                    ┘   └                        ┘   └      ┘    │
  │                                                                     │
  │  简化计算 (利用复数):                                               │
  │                                                                     │
  │  q' = q × e^(i·m·θ)                                                │
  │     = q × (cos(m·θ) + i·sin(m·θ))                                  │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘

  ┌─────────────────────────────────────────────────────────────────────┐
  │                                                                     │
  │  【2D RoPE 应用流程】                                               │
  │                                                                     │
  │  输入: Q, K ∈ R^(batch, seq_len, n_heads, head_dim)                │
  │        positions: [(row_0,col_0), (row_1,col_1), ..., (row_n,col_n)]│
  │                                                                     │
  │  Step 1: 计算频率                                                   │
  │  ┌─────────────────────────────────────────────────────────────┐   │
  │  │  freqs = 1.0 / (10000 ** (2k / half_dim))                   │   │
  │  │  k = 0, 1, 2, ..., half_dim/2 - 1                           │   │
  │  └─────────────────────────────────────────────────────────────┘   │
  │                                                                     │
  │  Step 2: 计算行和列的旋转角度                                       │
  │  ┌─────────────────────────────────────────────────────────────┐   │
  │  │  row_angles = row_positions × freqs    # 行旋转角度          │   │
  │  │  col_angles = col_positions × freqs    # 列旋转角度          │   │
  │  └─────────────────────────────────────────────────────────────┘   │
  │                                                                     │
  │  Step 3: 构建旋转矩阵 (或复数形式)                                  │
  │  ┌─────────────────────────────────────────────────────────────┐   │
  │  │  cos_row, sin_row = cos(row_angles), sin(row_angles)        │   │
  │  │  cos_col, sin_col = cos(col_angles), sin(col_angles)        │   │
  │  └─────────────────────────────────────────────────────────────┘   │
  │                                                                     │
  │  Step 4: 应用旋转到Q和K                                             │
  │  ┌─────────────────────────────────────────────────────────────┐   │
  │  │  Q[:, :, :, :half] = rotate(Q[:, :, :, :half], row_angles)  │   │
  │  │  Q[:, :, :, half:] = rotate(Q[:, :, :, half:], col_angles)  │   │
  │  │                                                             │   │
  │  │  K[:, :, :, :half] = rotate(K[:, :, :, :half], row_angles)  │   │
  │  │  K[:, :, :, half:] = rotate(K[:, :, :, half:], col_angles)  │   │
  │  └─────────────────────────────────────────────────────────────┘   │
  │                                                                     │
  │  注意: V 不应用RoPE!                                                │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘

  ┌─────────────────────────────────────────────────────────────────────┐
  │                                                                     │
  │  【2D RoPE 的优势】                                                 │
  │                                                                     │
  │  1. 相对位置编码: 注意力分数只依赖于相对位置差                      │
  │     Attention(q_m, k_n) ∝ q_m^T R_{m-n} k_n                        │
  │                                                                     │
  │  2. 2D空间感知: 模型能理解Patch在图像中的空间位置关系               │
  │                                                                     │
  │  3. 外推能力: 可以处理训练时未见过的更大图像                        │
  │                                                                     │
  │  4. 无需额外参数: 使用固定的正弦/余弦函数，不增加可学习参数          │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘

  ┌─────────────────────────────────────────────────────────────────────┐
  │                                                                     │
  │  【BOS Token 的位置处理】                                           │
  │                                                                     │
  │  BOS token 不属于图像的任何位置，可以:                              │
  │    方案A: 使用特殊位置 (-1, -1)，对应零旋转                         │
  │    方案B: 使用位置 (0, 0)，与第一个Patch共享位置                    │
  │    方案C: 不对BOS应用RoPE (mask掉)                                  │
  │                                                                     │
  │  推荐: 方案A，使用零旋转                                            │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘


================================================================================
【11. 可选扩展】
================================================================================

  ┌─────────────────────────────────────────────────────────────────────┐
  │                                                                     │
  │  1. 条件生成 (Conditional Generation):                             │
  │     ├── 类别条件: 添加class token                                  │
  │     ├── 文本条件: 添加text encoder                                 │
  │     └── 其他条件: 添加额外的embedding                              │
  │                                                                     │
  │  2. 更大的图像:                                                     │
  │     ├── 64×64: 1024个patch, 词表不变                               │
  │     ├── 128×128: 4096个patch, 序列更长                             │
  │     └── 需要更深的模型或更大的context window                        │
  │                                                                     │
  │  3. KV Cache (推理加速):                                           │
  │     └── 缓存已计算的key-value, 避免重复计算                         │
  │                                                                     │
  └─────────────────────────────────────────────────────────────────────┘


================================================================================
                                 END OF DIAGRAM
================================================================================